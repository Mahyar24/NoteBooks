{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f603a175-52d1-4eff-b4ed-a4e606405a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ebf1b02-b5fb-4d88-9d7d-4e4f09b77413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d543d423-d1d4-414d-b145-2fc8a4ba5f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_network(\n",
    "    input_size,\n",
    "    n_filters\n",
    "):\n",
    "\n",
    "    model = nn.Sequential()\n",
    "    \n",
    "    model.add_module('g_ConvT_1', nn.ConvTranspose2d(input_size, n_filters * 4, 4, 1, 0, bias=False))\n",
    "    model.add_module('g_bn_1', nn.BatchNorm2d(n_filters * 4))\n",
    "    model.add_module(f\"g_relu_1\", nn.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add_module('g_ConvT_2', nn.ConvTranspose2d(n_filters * 4, n_filters * 2, 3, 2, 1, bias=False))\n",
    "    model.add_module('g_bn_2', nn.BatchNorm2d(n_filters * 2))\n",
    "    model.add_module(f\"g_relu_2\", nn.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add_module('g_ConvT_3', nn.ConvTranspose2d(n_filters * 2, n_filters, 4, 2, 1, bias=False))\n",
    "    model.add_module('g_bn_3', nn.BatchNorm2d(n_filters))\n",
    "    model.add_module(f\"g_relu_3\", nn.LeakyReLU(0.2))\n",
    "    \n",
    "    model.add_module('g_ConvT_4', nn.ConvTranspose2d(n_filters, 1, 4, 2, 1, bias=False))\n",
    "    model.add_module(f\"g_tanh_4\", nn.Tanh())\n",
    "    \n",
    "    return model\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_filters):\n",
    "        super().__init__()\n",
    "        \n",
    "        model = nn.Sequential()\n",
    "        \n",
    "        model.add_module('d_Conv_1', nn.Conv2d(1, n_filters, 4, 2, 1, bias=False))\n",
    "        model.add_module('d_bn_1', nn.BatchNorm2d(n_filters))\n",
    "        model.add_module(f\"d_relu_1\", nn.LeakyReLU(0.2))\n",
    "\n",
    "        model.add_module('d_Conv_2', nn.Conv2d(n_filters, n_filters * 2, 4, 2, 1, bias=False))\n",
    "        model.add_module('d_bn_2', nn.BatchNorm2d(n_filters * 2))\n",
    "        model.add_module(f\"d_relu_2\", nn.LeakyReLU(0.2))\n",
    "\n",
    "        model.add_module('d_Conv_3', nn.Conv2d(n_filters * 2, n_filters * 4, 3, 2, 1, bias=False))\n",
    "        model.add_module('d_bn_3', nn.BatchNorm2d(n_filters * 4))\n",
    "        model.add_module(f\"d_relu_3\", nn.LeakyReLU(0.2))\n",
    "\n",
    "        model.add_module('d_Conv_4', nn.Conv2d(n_filters * 4, 1, 4, 1, 0, bias=False))\n",
    "        model.add_module(f\"d_sigm_4\", nn.Sigmoid())\n",
    "        \n",
    "        self.model = model\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90dc785d-1f7b-432f-a8de-7694f72fbfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13b66096-4e2e-44a9-978d-0493d9a4218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise(batch_size, z_size=100, mode='normal'):  # z_size == gen_model.input_size\n",
    "    if mode == 'normal':\n",
    "        return torch.randn(batch_size, z_size, 1, 1)\n",
    "    return (torch.rand(batch_size, z_size, 1, 1) * 2) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7088e78d-255f-4da6-ab04-c7e813715822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train(d_model, g_model, criterion, d_optimizer, x):\n",
    "    d_model.zero_grad()\n",
    "    # Train on REAL data\n",
    "    x = x.to(device)  # transfering data to GPU\n",
    "    d_labels_real = torch.ones(BATCH_SIZE, 1, device=device)\n",
    "    d_proba_real = d_model(x)\n",
    "    d_loss_real = criterion(d_proba_real, d_labels_real)\n",
    "    # Train on NOISE\n",
    "    noise = create_noise(BATCH_SIZE).to(device)\n",
    "    g_output = g_model(noise)\n",
    "    d_proba_fake = d_model(g_output)\n",
    "    d_labels_fake = torch.zeros(BATCH_SIZE, 1, device=device)\n",
    "    d_loss_fake = criterion(d_proba_fake, d_labels_fake)\n",
    "    # Backprop and optimize d_model only\n",
    "    d_loss = d_loss_real + d_loss_fake\n",
    "    d_loss.backward()\n",
    "    d_optimizer.step()\n",
    "    return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e36dbd3-d099-4692-aa3e-2c0fcd586fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train(g_model, d_model, criterion, g_optimizer):\n",
    "    g_model.zero_grad()\n",
    "    noise = create_noise(BATCH_SIZE).to(device)\n",
    "    g_output = g_model(noise)\n",
    "    d_proba_fake = d_model(g_output)\n",
    "    g_labels_real = torch.ones(BATCH_SIZE, 1, device=device)\n",
    "    g_loss = criterion(d_proba_fake, g_labels_real)\n",
    "    # Backprop and optimize g_model only\n",
    "    g_loss.backward()\n",
    "    g_optimizer.step()\n",
    "    return g_loss.data.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1be929c-05a1-4afb-aefd-e00854e2cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, d_model, criterion, g_optimizer, d_optimizer, data_loader, epochs=1):\n",
    "    all_d_losses = []\n",
    "    all_g_losses = []\n",
    "    all_d_real = []\n",
    "    all_d_fake = []\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        d_losses, g_losses = [], []\n",
    "        d_vals_real, d_vals_fake = [], []\n",
    "        \n",
    "        for i, (x, _) in enumerate(data_loader):\n",
    "            # Train and record d_model\n",
    "            d_loss, d_proba_real, d_proba_fake = discriminator_train(d_model, g_model, criterion, d_optimizer, x)\n",
    "            d_losses.append(d_loss)\n",
    "            d_vals_real.append(d_proba_real.mean().cpu())\n",
    "            d_vals_fake.append(d_proba_fake.mean().cpu())\n",
    "            # Train and record g_model\n",
    "            g_loss = generator_train(g_model, d_model, criterion, g_optimizer)\n",
    "            g_losses.append(g_loss)\n",
    "            \n",
    "        # Record for every epoch\n",
    "        all_d_losses.append(torch.tensor(d_losses).mean())\n",
    "        all_g_losses.append(torch.tensor(g_losses).mean())\n",
    "        all_d_real.append(torch.tensor(d_vals_real).mean())\n",
    "        all_d_fake.append(torch.tensor(d_vals_fake).mean())\n",
    "    \n",
    "        print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
    "              f' G/D {all_g_losses[-1]:.4f}/{all_d_losses[-1]:.4f}'\n",
    "              f' [D-Real: {all_d_real[-1]:.4f}'\n",
    "              f' D-Fake: {all_d_fake[-1]:.4f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6c4143d-4852-4000-bd89-08ad767ef52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "N_FILTERS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ceee7cd-bdf8-4e83-a524-3d7a42943e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST('./mnist/', train=True, download=False, transform=transformer)\n",
    "train_dl = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=8, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86f3eb70-0388-4f15-b26a-7c817383455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_model = make_generator_network(100, N_FILTERS).to(device)\n",
    "disc_model = Discriminator(N_FILTERS).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = torch.optim.Adam(gen_model.parameters(), lr=0.0003)\n",
    "d_optimizer = torch.optim.Adam(disc_model.parameters(), lr=0.0002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6161a981-f7f3-4306-aa74-32c288278729",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(gen_model, disc_model, criterion, g_optimizer, d_optimizer, train_dl, 1_000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
