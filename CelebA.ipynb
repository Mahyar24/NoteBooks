{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf7d16f-76e6-49e2-813b-f800cbd08cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import copy\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31912773-cab8-4e56-a7f9-9f1241a6f22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_train = T.Compose([\n",
    "    T.RandomCrop([178, 178]),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.Resize([64, 64]),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "transformer = T.Compose([\n",
    "    T.CenterCrop([178, 178]),\n",
    "    T.Resize([64, 64]),\n",
    "    T.ToTensor(),\n",
    "])\n",
    "\n",
    "get_smile = lambda attr: attr[18].to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23b974ca-b9a9-4f66-b15f-e4f6f34190d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CelebA('./', split='train', target_transform=get_smile, transform=transformer_train)\n",
    "valid_data = torchvision.datasets.CelebA('./', split='valid', target_transform=get_smile, transform=transformer)\n",
    "test_data = torchvision.datasets.CelebA('./', split='test', target_transform=get_smile, transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2bf65b9-fc7d-455f-a246-ac5e70c53d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowering number of samples\n",
    "train_data = Subset(train_data, torch.arange(16_000))\n",
    "valid_data = Subset(valid_data, torch.arange(1_000))\n",
    "test_data = Subset(test_data, torch.arange(1_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0797d61-f71b-4a84-bda4-a575c4984fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_dl = DataLoader(train_data, batch_size, shuffle=True, num_workers=8)\n",
    "valid_dl = DataLoader(valid_data, batch_size, shuffle=False, num_workers=8)\n",
    "test_dl = DataLoader(test_data, batch_size, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38fc7bd2-732c-44fb-8d09-680432074229",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        conv_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        conv_2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        conv_3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        conv_4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        convs = nn.Sequential(*[conv_1, conv_2, conv_3, conv_4])\n",
    "        \n",
    "        avg_flatten = nn.Sequential(\n",
    "            nn.AvgPool2d(kernel_size=8),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        fc_1 = nn.Sequential(\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        fc_2 = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "        fc_3 = nn.Linear(64, 1)\n",
    "        fcs = nn.Sequential(*[fc_1, fc_2, fc_3])\n",
    "        \n",
    "        sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        self.model = nn.Sequential(*[convs, avg_flatten, fcs, sigmoid])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84c917fa-7ebf-40ab-b34b-8518ebbf43c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, data_loader, num_epochs, epoch_print=1):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        if epoch == 1 or epoch % epoch_print == 0:\n",
    "            print(f'\\nEpoch {epoch}/{num_epochs}')\n",
    "            print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in data_loader[phase]:\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)[:, 0]\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum((outputs > 0.5) == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(data_loader[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(data_loader[phase].dataset)\n",
    "\n",
    "            if epoch == 1 or epoch % epoch_print == 0:\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "                \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c462370e-881a-47f8-a888-3e0a6efca80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "947dd6cb-8807-465e-9f4c-2176d8616fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n",
      "----------\n",
      "train Loss: 0.5561 Acc: 0.7061\n",
      "val Loss: 0.7070 Acc: 0.6430\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n",
      "train Loss: 0.4197 Acc: 0.8043\n",
      "val Loss: 0.4346 Acc: 0.8030\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n",
      "train Loss: 0.3316 Acc: 0.8531\n",
      "val Loss: 0.3625 Acc: 0.8430\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n",
      "train Loss: 0.2848 Acc: 0.8723\n",
      "val Loss: 0.7092 Acc: 0.6740\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n",
      "train Loss: 0.2666 Acc: 0.8839\n",
      "val Loss: 1.0630 Acc: 0.6270\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n",
      "train Loss: 0.2475 Acc: 0.8922\n",
      "val Loss: 0.3316 Acc: 0.8530\n",
      "\n",
      "Epoch 7/20\n",
      "----------\n",
      "train Loss: 0.2418 Acc: 0.8934\n",
      "val Loss: 0.6636 Acc: 0.7450\n",
      "\n",
      "Epoch 8/20\n",
      "----------\n",
      "train Loss: 0.2314 Acc: 0.8989\n",
      "val Loss: 0.3352 Acc: 0.8570\n",
      "\n",
      "Epoch 9/20\n",
      "----------\n",
      "train Loss: 0.2337 Acc: 0.8962\n",
      "val Loss: 0.8579 Acc: 0.6780\n",
      "\n",
      "Epoch 10/20\n",
      "----------\n",
      "train Loss: 0.2249 Acc: 0.9019\n",
      "val Loss: 0.2894 Acc: 0.8710\n",
      "\n",
      "Epoch 11/20\n",
      "----------\n",
      "train Loss: 0.2226 Acc: 0.9029\n",
      "val Loss: 0.2658 Acc: 0.8900\n",
      "\n",
      "Epoch 12/20\n",
      "----------\n",
      "train Loss: 0.2175 Acc: 0.9059\n",
      "val Loss: 0.5089 Acc: 0.8130\n",
      "\n",
      "Epoch 13/20\n",
      "----------\n",
      "train Loss: 0.2129 Acc: 0.9061\n",
      "val Loss: 0.2963 Acc: 0.8670\n",
      "\n",
      "Epoch 14/20\n",
      "----------\n",
      "train Loss: 0.2117 Acc: 0.9055\n",
      "val Loss: 0.7204 Acc: 0.7450\n",
      "\n",
      "Epoch 15/20\n",
      "----------\n",
      "train Loss: 0.2096 Acc: 0.9069\n",
      "val Loss: 0.5746 Acc: 0.7830\n",
      "\n",
      "Epoch 16/20\n",
      "----------\n",
      "train Loss: 0.2006 Acc: 0.9119\n",
      "val Loss: 0.7932 Acc: 0.7610\n",
      "\n",
      "Epoch 17/20\n",
      "----------\n",
      "train Loss: 0.2051 Acc: 0.9119\n",
      "val Loss: 0.2458 Acc: 0.8880\n",
      "\n",
      "Epoch 18/20\n",
      "----------\n",
      "train Loss: 0.1965 Acc: 0.9136\n",
      "val Loss: 0.3053 Acc: 0.8630\n",
      "\n",
      "Epoch 19/20\n",
      "----------\n",
      "train Loss: 0.1976 Acc: 0.9127\n",
      "val Loss: 0.2912 Acc: 0.8790\n",
      "\n",
      "Epoch 20/20\n",
      "----------\n",
      "train Loss: 0.1942 Acc: 0.9147\n",
      "val Loss: 0.6524 Acc: 0.7960\n",
      "Training complete in 30m 8s\n",
      "Best val Acc: 0.890000\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, {'train': train_dl, 'val': valid_dl}, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b130fc26-404f-4b7b-ae29-17c60909450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, labels in loader:model = train_model(model, criterion, optimizer, {'train': train_dl, 'val': valid_dl}, 20)\n",
    "            \n",
    "            outputs = model(x)[:, 0]\n",
    "            num_correct += torch.sum((outputs > 0.5) == labels.data)\n",
    "            num_samples += outputs.size(0)\n",
    "        \n",
    "        print(f'Got {num_correct} / {num_samples} with accuracy {float(num_correct)/float(num_samples)*100:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e187ef5-d9ac-4b81-932f-a7237f6be19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 895 / 1000 with accuracy 89.50\n"
     ]
    }
   ],
   "source": [
    "check_accuracy(test_dl, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73a176e7-ae96-4436-97ac-2d3bdf4a5866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471681"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
